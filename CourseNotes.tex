\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\begin{document}

\section*{Computational Statistics}

This book contains the collection of lecture notes and personal anecdotes from Gabriel Chandler's Computational Statistics course (MATH 154). 

\section{Density Estimation}
Recall from introductory statistics: one of the most common goals of a statistician is predicting what a population might look like based on a random sample from that population. \\

More rigorously, a random variable $X$ is a function that takes an element in the sample space and maps it to a real number. For example, when referring to a sample population of 'students', then the function of measuring a group of their heights would be one such random variable of 'students'. (Of course, there may also be numerous other random variables that exist in this sample space, such as hair color or weight or grades). \\

We are often tasked with estimating the distribution of these random variables. These variables can be either discrete values or continuous values. \\

Estimating the distribution of discrete values is pretty straightforward: simply find the proportion/percentage of times that $k$ appears in a random sample. This will yield us $P(X = k)$. This method is called the probability mass function (PMF) and is simply represented as: 


$$\hat{p}(x) = \sum \frac{\mathbf{1}(x_i = k)}{n}$$ 

(Note that $\mathbf{1}$ simply refers to the function that returns 1 if the inner boolean is satisfied and returns 0 if the inner boolean is false). \\

However, the above strategy cannot be used for continuous distributions. That is because there is an infinite number of values on any subsection of the real number line, so the probability that any specific value appears more than once when plucked from any continuous distribution is effectively zero. In fact, $P(X = k) = 0$ technically for all values $k$ on any part of the real number line. (Silly illustrating example: How many people in the world are EXACTLY 6 feet tall? Probably none, because one person might technically be 6.001... feet tall or 5.999... feet tall, or so on). \\

Instead when working on any continuous space, we must instead work in terms of intervals. Alternatively, we can think about the probablity that a value falls within a certain range $P(a < X < b)$. (Continuing example: We may not be able to find anyone who is exactly 6 feet tall, but instead we can find a reasonable amount of people who are between 5'11.5" feet tall and 6'0.5" feet tall, which also happens to be what we mean colloquially when we say someone is 6 feet tall). \\

We want to create a function to where we can select any subinterval that we may need for our distribution. We will thus call this function the density function $f(x)$, where $P(a < X < b) = \int_{a}^{b} f(x) dx$. Because the density function outputs the probability that a given value lies within that interval, then when we integrate our entire function $f(x)$ then it must equal 1. \\

So if $f(x)$ is our 'perfect' or 'true population' density function, what are some of the ways that we can estimate this function? \\

\subsection{Indicator Function Estimation}

Our first method of making a density estimator might be to adapt our density estimator for discrete distributions. Why can't a model like this do the trick?

$$\hat{f}_X(x) = \frac{1}{n} \mathbf{1}[ X_i = x ]$$

This function gives a probability of 0 if the value hasn't been observed or gives a probability of 1 if the value has been observed. Now this estimator is problematic for an number of reasons. First off: it's going to always going to return 0 unless the value has been seen before - and since we're in a continuous distribution where every observation is going to be a new value, then it's just always going to return 0 for every new value. \\

In theory, this estimator actually maximizes the probability definition of likelihood for your data. However, we will soon see that good or perfect results on your sample population (or training set) doesn't mean a function will generalize well to the rest of the population. Remember that any randomly-generated datapoint consists of two parts: signal and noise. If you make your model fit too closely to your training data, then your model learns more undesirable noise than meaningful signal. \\

What are some ways we can account for this randomness?

\subsection{Histograms and a Slightly Better Estimator}

You've probably heard of it: the histogram is a common tool used in exploratory data analysis and visualization that works as follows: break up your dataset into equal-sized intervals called 'bins', and count up the number of observations that falls under each bin. If you rescale the dataset so that the sum of the area of all the bins is equal to 1, then we actually have a density estimator: the 'height' of each bin is the density, or the probability that a given value falls under that interval. \\

Histograms are a great start to density estimation but they do introduce a number of problems. One is subjectivity: choices such as number of bins, bin size, and start/stop values of bin boundaries introduce the element of choice which can (intentionally or unintentionally) skew the data. For example: you can pluck random values from the normal distribution a histogram, and depending on where you start and stop the bins, you can make the data appear to be either left- or right-skewed. \\

The second main problem of the histogram is its lack of 'smoothness'. Most random distributions in nature tend to be smooth 'smooth'. There is no precise definition of smoothness, but we can say that is approximately means that if we observe a value $x$ in the dataset, then it is reasonable to assume that another value near $y$ exists nearby on that function. We can go further and say that a 'smooth' function is one that is continuous, or even one that is differentiable. No spikes, corners, or jagged edges. (Ending our height analogy, it would probably be inaccurate to have a function that says there are lots of people that are 5'9" and 5'11", but 5'10" people don't exist). \\

With that in mind, we do like the histogram because of how it is able to predict that if there are a lot of nearby values in one location, then the density will be high, and if there are few values in another location, then the density at that location will be low. With that, maybe we can construct the following estimator: 

$$ \hat{f}(x) = \frac{1}{nh} \sum \mathbf{1} (|x_i - x| < \frac{h}{2}) $$

The function above looks at a given value $x$, and checks to see how many data points $x_i$ are nearby (nearby being defined as less than our value $h$, which we can call our bandwidth or tolerance). The more nearby values, the higher the density, and the less nearby values, the lower the densty. Not perfect, but a good start.

\subsection{Kernel Density Estimators}

Our current estimator works somewhat well if we know the $h$ value. But it's not always easy in nature to know what the best $h$ is - to do so we would have to know the distribution and would defeat the purpose of needing to model a density estimate in the first place. \\

We will introduce the idea of a kernel. The last model we used was currently using is called a 'box kernel' - if we are testing a particular value, then our current box kernel simply returns 1 for each data point within the box and 0 for each data point out of the box. It does NOT take into account how close or far away each datapoint is from the value we are testing. \\

So instead of simply limiting ourselves to our box kernel, we will expand this to any kernel function $K$ in our density estimator: 

$$ \hat{f}_h(x) = \frac{1}{nh} \sum K (\frac{x_i - x}{h}) $$

For a kernel function to be valid, it must satisfy two things: one, $ \int K(u)du$ must equal 1 for our resulting function to be a density. It must also be unimodal and symmetrical with a mean of 0. This allows us to input, say, the normal distribution, which penalizes each datapoint in proportion to how far away it is from the value we are testing. \\

However we are still left with the same fundamental problem: how do we choose a good $h$?

\subsection{The Bias-Variance Tradeoff}

The bias-variance tradeoff is one of the most important concepts in the field of machine learning. Generally, it's this tradeoff that usually determines why we select certain parameters or models. The bias-variance tradeoff appears in all forms of statistical learning, such as regression and classification, but with the density problem, most the tradeoff comes from our choice of $h$, our bandwidth. \\

Remember that $h$ is the parameter that detects how far our kernel is able to see when looking at each datapoint. An overly large $h$ gives us a large interval that is too sensitive and will be able to pick up irrelevant information from datapoints far away from any given $x_i$. This results in density estimators with high \textbf{bias} - incorrect predictions that come from irrelevant information - but low variance, as reproducing this density estimator with a different sample from the same population will yield similar results. \\

By contrast, when we have too small $h$, then we are only capturing information that is extremely close to our datapoint. Think of it as us having a very 'picky' bandwidth. Having a picky bandwidth is useful in the sense that it decreases our chance of picking up irrelevant information, but on the flipside, our bandwidth might be so picky that it may also fail to pick up all of the useful information. This results in high \textbf{variance}, but low bias - the information that we have is useful and accurate, but we just don't have enough of it to create an accurate model. Note that taking a lot of high variance, low bias models and averaging them all together would result in a more accurate model by constructing the 'full picture' of the signal around it. \\

Note that when $h$ is infinitely small or $h = 0$, we essentially have our indicator function estimator, which is in fact so picky that it can only predict data that has been seen before. When $h$ is infinitely large, we will just end up with a straight line constant.


\subsection{Maximum Likelihood Estimation and Leave-One-Out Cross Valdation}

Using the indicator function or maximum likelihood estimator to estimate data isn't completely useless. The reason it isn't helpful is because it predicts our known data well, but says that future data is impossible. But instead what if we could tweak it to maximize future data, and make a 'maximum future likelihood estimator' function?

$$L_f(x) = \prod f(x^*_j)$$ 

Well, unfortunately knowing future data is impossible, otherwise we wouldn't be doing statistics. But instead, we can do something clever, and 'pretend' we have future data by intentionally withholding some data from our model. This is called splitting our training data into \textbf{test data}. Then we can run our maximum likelihood estimator on our 'known data', and see how well it predicts our 'test data' or 'future data'. 

\section{Regression}

Remember that regression is the process by which we estimate some 'dependent' variable based on one or several related 'independent' variables. Simple univariate regression is usually of the form: 

$$y_i = f(x_i) + \epsilon $$ \

where $y_i$ is our predicted value, $f()$ is the actual generating function, $x_i$ is our independent or 'known' variable, and $\epsilon$ is some random error (usually assumed mean 0). The goal of regression is to be able to create some estimate for $f()$, given a pre-existing training set. Our function $f()$ can then be used to make predictions on batches of fresh data.\\

\subsection{Kernel Regression Smoothing}
One way to create regression models: what if we take the concept we used for a density estimator, and use it as a regression estimator? Using similar methodology to our KDE, we can use a weighted system to make predictions, where we look at each point, and calculate a score using the proximity of nearby points, weighing close points higher and weighing far-away points lower. This weighting methodology is called \textbf{LOESS}. This results in a kernel smoothing regression that looks something like this:

$$\hat{f}(x) = \frac { \sum y_i K (\frac{x_i-x}{h}) } { \sum K (\frac{x_i-x}{h}) } $$ \

Kernel regression smoothing works okay in some scenarios, but presents a lot of problems. First problem: similar to KDE, we are required to select parameter $h$. If $h$ is way too small, we will overfit, or 'connect the dots' with our points. I $h$ is way too big, then we will underfit, and just get a non-helpful straight line. We can at least solve this problem with cross-validation. (We also need to select a kernel $K(\dot)$ but this tends to not be super important).\\

But the bigger problem presented by kernel smoothing is its questionable effectiveness in higher dimensins. Note that we calculate weighting using 'closeness', and closeness is defined using distance. Distance becomes an increasingly useless metric in higher and higher dimensions. This is called the \textbf{curse of dimensionality}, in which the distance between points grow further and further apart. This creates a 'sparse' dataset which has a number of undesirable properties: for example, certain datapoints called 'hubs' become the nearest neighbour of a huge percentage of the other points in the dataset. Such a machine learning model used by Pandora for example may keep recommending the same songs over and over and over again. \\


\subsection{Additive Models and Backfitting Algorithm}

One solution for the curse of dimensionality is to assume our model is an \textbf{additive model}, meaning it is of the form:

$$ y = \beta_0 + \sum f_i(x_i) + \epsilon$$ 

We can compress high dimensional data into a unidimensional form using the \textbf{backfitting algorithm}. The backfitting algorithm allows you to use a very smart learner like kernel regression only in one dimension at a time (so long as you are running it under the assumption that the target function is additive), which avoids some of the issues that come with high dimensionality.  


\subsection{Linear Regression}

Assuming our data is 'additive' is a pretty damn strong assumption. Instead, we can use linear regression. Although it's weird because technically assuming our data is 'linear' seems like a strognger assumption than additive, but actually we will soon see that through linear transformations we can actually be very flexible\\



\end{document}
